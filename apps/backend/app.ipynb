{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2e03236-6d23-41ad-8eb8-79849a77715f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# except ImportError:\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# from langchain.text_splitter import RecursiveCharacterTextSplitter\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from langchain.text_splitters import RecursiveCharacterTextSplitter\n",
    "# except ImportError:\n",
    "    # from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\", \"YOUR_PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX\", \"YOUR_INDEX_NAME\")\n",
    "data_folder = os.getenv(\"DATA_DIR\", r\"YOUR_LOCAL_DATASET_DIRECTORY_PATH\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")  # 1536 dims\n",
    "\n",
    "# Resume Support\n",
    "start_batch = int(os.getenv(\"START_BATCH\", \"0\"))  # Can be overridden via env\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\", \"100\"))  # Batch embeddings to reduce API calls\n",
    "\n",
    "# Initialize Clients\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    lines = [line.strip() for line in s.split(\"\\n\")]\n",
    "    # Remove empty lines to compact chunks\n",
    "    non_empty = [ln for ln in lines if ln]\n",
    "    return \"\\n\".join(non_empty)\n",
    "\n",
    "def read_txt(file_path: str) -> str | None:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "        text = normalize_text(text)\n",
    "        return text if text else None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to read TXT {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_strings_from_json(obj) -> list[str]:\n",
    "    preferred_keys = {\"text\", \"content\", \"context\", \"body\", \"chunk\", \"page_content\"}\n",
    "    preferred, others = [], []\n",
    "\n",
    "    def walk(o, parent_key=None):\n",
    "        if isinstance(o, dict):\n",
    "            for k, v in o.items():\n",
    "                walk(v, k)\n",
    "        elif isinstance(o, list):\n",
    "            for item in o:\n",
    "                walk(item, parent_key)\n",
    "        elif isinstance(o, str):\n",
    "            if parent_key in preferred_keys:\n",
    "                preferred.append(o)\n",
    "            else:\n",
    "                others.append(o)\n",
    "\n",
    "    walk(obj)\n",
    "    return preferred if preferred else others\n",
    "\n",
    "def read_json(file_path: str) -> str | None:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            data = json.load(f)\n",
    "        strings = extract_strings_from_json(data)\n",
    "        if not strings:\n",
    "            return None\n",
    "        text = normalize_text(\"\\n\".join(strings))\n",
    "        return text if text else None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to read JSON {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def iter_data_files(root_dir: str):\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for name in filenames:\n",
    "            lower = name.lower()\n",
    "            if lower.endswith(\".txt\") or lower.endswith(\".json\"):\n",
    "                yield os.path.join(dirpath, name)\n",
    "\n",
    "# Chunk Preparation\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,      # slightly larger chunks for better throughput\n",
    "    chunk_overlap=100,   # moderate overlap for context continuity\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "seen_hashes = set()\n",
    "\n",
    "for file_path in iter_data_files(data_folder):\n",
    "    try:\n",
    "        if file_path.lower().endswith(\".txt\"):\n",
    "            text = read_txt(file_path)\n",
    "        else:\n",
    "            text = read_json(file_path)\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        splits = splitter.split_text(text)\n",
    "        for chunk in splits:\n",
    "            chunk = chunk.strip()\n",
    "            if not chunk:\n",
    "                continue\n",
    "            h = hashlib.sha1(chunk.encode(\"utf-8\")).hexdigest()\n",
    "            if h in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(h)\n",
    "            all_chunks.append({\n",
    "                \"id\": f\"{os.path.basename(file_path)}-{h[:12]}\",\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": os.path.relpath(file_path, data_folder).replace(\"\\\\\", \"/\"),\n",
    "                    \"hash\": h\n",
    "                }\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {file_path}: {e}\")\n",
    "\n",
    "if not all_chunks:\n",
    "    print(\"⚠️ No chunks prepared. Check your data directory and file formats.\")\n",
    "else:\n",
    "    print(f\"Prepared {len(all_chunks)} unique chunks from {data_folder}\")\n",
    "\n",
    "# -------- Embeddings (batched) --------\n",
    "def get_embeddings_batch(texts: list[str], retries: int = 3, delay: int = 5) -> list | None:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = client.embeddings.create(\n",
    "                model=embedding_model,\n",
    "                input=texts\n",
    "            )\n",
    "            # Align by index\n",
    "            return [d.embedding for d in resp.data]\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Embedding batch failed (attempt {attempt + 1}): {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "    print(\"❌ Giving up on this embedding batch.\")\n",
    "    return None\n",
    "\n",
    "# -------- Upload to Pinecone in Batches with Resume Support --------\n",
    "total_batches = (len(all_chunks) + batch_size - 1) // batch_size\n",
    "for i in range(start_batch * batch_size, len(all_chunks), batch_size):\n",
    "    batch_index = i // batch_size\n",
    "    batch = all_chunks[i:i + batch_size]\n",
    "    print(f\"Uploading batch {batch_index + 1} / {total_batches} (size={len(batch)})\")\n",
    "\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    embeddings = get_embeddings_batch(texts)\n",
    "\n",
    "    if embeddings is None:\n",
    "        print(f\"❌ Skipping upload for batch {batch_index + 1} due to embedding failure.\")\n",
    "        print(\"❗ Stopping script to prevent data inconsistency. Update START_BATCH to resume.\")\n",
    "        break\n",
    "\n",
    "    vectors = []\n",
    "    for item, emb in zip(batch, embeddings):\n",
    "        if emb is None:\n",
    "            continue\n",
    "        vectors.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"values\": emb,\n",
    "            \"metadata\": item[\"metadata\"]\n",
    "        })\n",
    "\n",
    "    if not vectors:\n",
    "        print(\"⚠️ No vectors in this batch to upload.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"✅ Uploaded {len(vectors)} vectors.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload batch {batch_index + 1}: {e}\")\n",
    "        print(\"❗ Stopping script to prevent data loss. Please resume by updating START_BATCH.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(0.5)  # gentle pacing to reduce rate-limit risk\n",
    "\n",
    "print(\"✅ Script execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4adb6ee-7455-43b3-b03f-7df46ae03428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
