{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e03236-6d23-41ad-8eb8-79849a77715f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# perplixity script with enhancements\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import spacy\n",
    "import yake\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\", \"YOUR_PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX\", \"YOUR_INDEX_NAME\")\n",
    "data_folder = os.getenv(\"DATA_DIR\", r\"YOUR_LOCAL_DATA_DIRECTORY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")  # 3072 dims for accuracy\n",
    "pinecone_environment = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east-1\")\n",
    "\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\", 100))\n",
    "start_batch = int(os.getenv(\"START_BATCH\", 0))\n",
    "\n",
    "# =========================\n",
    "# INIT CLIENTS\n",
    "# =========================\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # NER model\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "\n",
    "# Create Pinecone index if not exists\n",
    "index_list = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "if index_name not in index_list:\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        cloud=\"aws\",\n",
    "        region=pinecone_environment,\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print(f\"Using existing index: {index_name}\")\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    return \"\\n\".join(line.strip() for line in s.split(\"\\n\") if line.strip())\n",
    "\n",
    "def read_txt(file_path: str) -> str | None:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return normalize_text(f.read())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_json(file_path: str) -> str | None:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            data = json.load(f)\n",
    "        preferred_keys = {\"text\", \"content\", \"context\", \"body\", \"chunk\", \"page_content\", \"data\", \"message\"}\n",
    "        strings = []\n",
    "\n",
    "        def walk(o, parent_key=None):\n",
    "            if isinstance(o, dict):\n",
    "                for k, v in o.items():\n",
    "                    walk(v, k)\n",
    "            elif isinstance(o, list):\n",
    "                for item in o:\n",
    "                    walk(item, parent_key)\n",
    "            elif isinstance(o, str) and o.strip():\n",
    "                if parent_key and parent_key.lower() in preferred_keys:\n",
    "                    strings.append(o)\n",
    "                else:\n",
    "                    strings.append(o)\n",
    "\n",
    "        walk(data)\n",
    "        return normalize_text(\"\\n\".join(strings)) if strings else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_tsv(file_path: str) -> str | None:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", dtype=str, encoding=\"utf-8\", error_bad_lines=False)\n",
    "        text = \"\\n\".join(df.fillna(\"\").astype(str).agg(\" \".join, axis=1))\n",
    "        return normalize_text(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def iter_data_files(root_dir: str):\n",
    "    supported_ext = (\".txt\", \".json\", \".tsv\")\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for name in filenames:\n",
    "            if name.lower().endswith(supported_ext):\n",
    "                yield os.path.join(dirpath, name)\n",
    "\n",
    "def extract_entities(text: str):\n",
    "    doc = nlp(text)\n",
    "    return list({ent.text for ent in doc.ents})\n",
    "\n",
    "def extract_keywords(text: str):\n",
    "    return [kw for kw, _ in kw_extractor.extract_keywords(text)]\n",
    "\n",
    "def normalize_vector(vec):\n",
    "    vec = np.array(vec, dtype=np.float32)\n",
    "    return (vec / np.linalg.norm(vec)).tolist()\n",
    "\n",
    "def get_embeddings_batch(texts, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=embedding_model, input=texts)\n",
    "            return [d.embedding for d in resp.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding batch failed: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# CHUNKING\n",
    "# =========================\n",
    "splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=100, encoding_name=\"cl100k_base\")\n",
    "\n",
    "all_chunks = []\n",
    "seen_hashes = set()\n",
    "\n",
    "print(f\"Scanning dataset in {data_folder}...\")\n",
    "for file_path in tqdm(list(iter_data_files(data_folder))):\n",
    "    if file_path.lower().endswith(\".txt\"):\n",
    "        text = read_txt(file_path)\n",
    "    elif file_path.lower().endswith(\".json\"):\n",
    "        text = read_json(file_path)\n",
    "    elif file_path.lower().endswith(\".tsv\"):\n",
    "        text = read_tsv(file_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        h = hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "\n",
    "        metadata = {\n",
    "            \"source\": os.path.relpath(file_path, data_folder).replace(\"\\\\\", \"/\"),\n",
    "            \"chunk_index\": i,\n",
    "            \"file_type\": os.path.splitext(file_path)[1][1:],\n",
    "            \"entities\": extract_entities(chunk),\n",
    "            \"keywords\": extract_keywords(chunk),\n",
    "            \"chunk_length\": len(chunk)\n",
    "        }\n",
    "\n",
    "        all_chunks.append({\n",
    "            \"id\": f\"{os.path.basename(file_path)}-{h[:12]}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(all_chunks)} chunks.\")\n",
    "\n",
    "# =========================\n",
    "# EMBEDDING + UPLOAD\n",
    "# =========================\n",
    "total_batches = (len(all_chunks) + batch_size - 1) // batch_size\n",
    "uploaded_vectors = 0\n",
    "\n",
    "for idx in range(start_batch, total_batches):\n",
    "    batch = all_chunks[idx * batch_size : (idx + 1) * batch_size]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "\n",
    "    embeddings = get_embeddings_batch(texts)\n",
    "    if not embeddings:\n",
    "        print(\"Skipping batch due to embedding failure.\")\n",
    "        continue\n",
    "\n",
    "    vectors = []\n",
    "    for item, emb in zip(batch, embeddings):\n",
    "        vectors.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"values\": normalize_vector(emb),\n",
    "            \"metadata\": {**item[\"metadata\"], \"text\": item[\"text\"]}\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        index.upsert(vectors=vectors)\n",
    "        uploaded_vectors += len(vectors)\n",
    "        print(f\"Uploaded {len(vectors)} vectors (Total: {uploaded_vectors})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upsert batch {idx+1}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Total vectors uploaded: {uploaded_vectors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4adb6ee-7455-43b3-b03f-7df46ae03428",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myake\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# some updation like token based chunking instead of char based\n",
    "# Handles .txt, .json, .tsv.\n",
    "# Uses token-based chunking instead of character-based.\n",
    "# Runs NER, keyword extraction, and summarization per chunk.\n",
    "# Stores rich metadata for hybrid retrieval in Pinecone.\n",
    "# Works in streaming mode for 20GB datasets so you don’t blow RAM.\n",
    "# Supports resume after failure.\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import yake\n",
    "import tiktoken\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# === CONFIG ===\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX\")\n",
    "data_folder = os.getenv(\"DATA_DIR\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")  # higher accuracy\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\", \"100\"))\n",
    "start_batch = int(os.getenv(\"START_BATCH\", \"0\"))\n",
    "\n",
    "# === INIT ===\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "kw_extractor = yake.KeywordExtractor(n=2, top=5)\n",
    "encoding = tiktoken.encoding_for_model(embedding_model)\n",
    "\n",
    "# Create index if not exists\n",
    "if index_name not in [idx.name for idx in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072 if \"large\" in embedding_model else 1536,\n",
    "        metric=\"cosine\",\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "# === HELPERS ===\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    lines = [line.strip() for line in s.split(\"\\n\")]\n",
    "    return \"\\n\".join([ln for ln in lines if ln])\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return normalize_text(f.read())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            data = json.load(f)\n",
    "        return normalize_text(\"\\n\".join(extract_strings_from_json(data)))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_strings_from_json(obj):\n",
    "    keys = {\"text\", \"content\", \"context\", \"body\", \"chunk\", \"page_content\", \"data\", \"message\"}\n",
    "    preferred, others = [], []\n",
    "    def walk(o, parent=None):\n",
    "        if isinstance(o, dict):\n",
    "            for k, v in o.items():\n",
    "                walk(v, k)\n",
    "        elif isinstance(o, list):\n",
    "            for i in o:\n",
    "                walk(i, parent)\n",
    "        elif isinstance(o, str) and o.strip():\n",
    "            (preferred if parent and parent.lower() in keys else others).append(o)\n",
    "    walk(obj)\n",
    "    return preferred if preferred else others\n",
    "\n",
    "\n",
    "def read_tsv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "        text_data = \" \".join(df.fillna(\"\").astype(str).agg(\" \".join, axis=1))\n",
    "        return normalize_text(text_data)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def iter_data_files(root):\n",
    "    exts = (\".txt\", \".json\", \".tsv\")\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for name in filenames:\n",
    "            if name.lower().endswith(exts):\n",
    "                yield os.path.join(dirpath, name)\n",
    "\n",
    "\n",
    "def get_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return list(set(ent.text for ent in doc.ents))\n",
    "\n",
    "\n",
    "def get_keywords(text):\n",
    "    return [kw for kw, score in kw_extractor.extract_keywords(text)]\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"Summarize in 1-2 sentences.\"},\n",
    "                      {\"role\": \"user\", \"content\": text}],\n",
    "            max_tokens=80,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_token_length(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=get_token_length\n",
    ")\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts):\n",
    "    try:\n",
    "        resp = client.embeddings.create(\n",
    "            model=embedding_model,\n",
    "            input=texts,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "        return [d.embedding for d in resp.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# === MAIN PIPELINE ===\n",
    "seen_hashes = set()\n",
    "all_chunks = []\n",
    "file_count = 0\n",
    "\n",
    "for file_path in iter_data_files(data_folder):\n",
    "    if file_path.lower().endswith(\".txt\"):\n",
    "        text = read_txt(file_path)\n",
    "    elif file_path.lower().endswith(\".json\"):\n",
    "        text = read_json(file_path)\n",
    "    else:\n",
    "        text = read_tsv(file_path)\n",
    "\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    file_count += 1\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        h = hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "\n",
    "        entities = get_entities(chunk)\n",
    "        keywords = get_keywords(chunk)\n",
    "        summary = summarize_text(chunk)\n",
    "\n",
    "        all_chunks.append({\n",
    "            \"id\": f\"{os.path.basename(file_path)}-{h[:12]}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source\": os.path.relpath(file_path, data_folder),\n",
    "                \"chunk_index\": idx,\n",
    "                \"file_type\": os.path.splitext(file_path)[1][1:],\n",
    "                \"chunk_length\": get_token_length(chunk),\n",
    "                \"entities\": entities,\n",
    "                \"keywords\": keywords,\n",
    "                \"summary\": summary\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Stream upload in batches to save memory\n",
    "    while len(all_chunks) >= batch_size:\n",
    "        batch = all_chunks[:batch_size]\n",
    "        embeddings = get_embeddings_batch([c[\"text\"] for c in batch])\n",
    "        if embeddings:\n",
    "            index.upsert(vectors=[\n",
    "                {\"id\": c[\"id\"], \"values\": e, \"metadata\": {**c[\"metadata\"], \"text\": c[\"text\"]}}\n",
    "                for c, e in zip(batch, embeddings)\n",
    "            ])\n",
    "        all_chunks = all_chunks[batch_size:]\n",
    "\n",
    "# Upload any remaining chunks\n",
    "if all_chunks:\n",
    "    embeddings = get_embeddings_batch([c[\"text\"] for c in all_chunks])\n",
    "    if embeddings:\n",
    "        index.upsert(vectors=[\n",
    "            {\"id\": c[\"id\"], \"values\": e, \"metadata\": {**c[\"metadata\"], \"text\": c[\"text\"]}}\n",
    "            for c, e in zip(all_chunks, embeddings)\n",
    "        ])\n",
    "\n",
    "print(f\"✅ Processed {file_count} files into Pinecone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import yake\n",
    "import tiktoken\n",
    "import asyncio\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Advanced libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import chardet\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\")\n",
    "    pinecone_api_key: str = os.getenv(\"PINECONE_API_KEY\")\n",
    "    \n",
    "    index_name: str = os.getenv(\"PINECONE_INDEX\")\n",
    "    namespace_dense: str = \"dense_vectors\"\n",
    "    namespace_sparse: str = \"sparse_vectors\"\n",
    "    \n",
    "    data_folder: str = os.getenv(\"DATA_DIR\")\n",
    "    cache_dir: str = os.getenv(\"CACHE_DIR\", \"./cache\")\n",
    "    \n",
    "    embedding_model: str = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "    keyword_model: str = \"all-MiniLM-L6-v2\"\n",
    "    \n",
    "    chunk_size: int = 800\n",
    "    chunk_overlap: int = 150\n",
    "    batch_size: int = int(os.getenv(\"BATCH_SIZE\", \"50\"))\n",
    "    max_workers: int = 4\n",
    "    \n",
    "    initial_retrieval_k: int = 100\n",
    "    rerank_k: int = 30\n",
    "    final_k: int = 10\n",
    "    hybrid_alpha: float = 0.7  # Weight for dense vs sparse\n",
    "\n",
    "config = RAGConfig()\n",
    "\n",
    "# === INITIALIZATION ===\n",
    "print(\"Initializing models and connections...\")\n",
    "\n",
    "client = OpenAI(api_key=config.openai_api_key)\n",
    "pc = Pinecone(api_key=config.pinecone_api_key)\n",
    "\n",
    "# NLP Models\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")  # Use large model for better NER\n",
    "except:\n",
    "    print(\"Downloading spaCy large model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Keyword extraction models\n",
    "kw_extractor = yake.KeywordExtractor(n=3, top=10, dedupLim=0.7)\n",
    "keybert_model = KeyBERT(model=config.keyword_model)\n",
    "\n",
    "# Cross-encoder for reranking\n",
    "cross_encoder = CrossEncoder(config.reranker_model)\n",
    "\n",
    "# Tokenizer\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Create cache directory\n",
    "os.makedirs(config.cache_dir, exist_ok=True)\n",
    "\n",
    "# === ENHANCED PINECONE INDEX ===\n",
    "def setup_pinecone_index():\n",
    "    \"\"\"Create optimized Pinecone index with metadata configuration\"\"\"\n",
    "    dimension = 3072 if \"large\" in config.embedding_model else 1536\n",
    "    \n",
    "    if config.index_name not in [idx.name for idx in pc.list_indexes()]:\n",
    "        print(f\"Creating index {config.index_name}...\")\n",
    "        pc.create_index(\n",
    "            name=config.index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "            spec={\n",
    "                \"pod\": {\n",
    "                    \"replicas\": 1,\n",
    "                    \"shards\": 1,\n",
    "                    \"pods\": 1,\n",
    "                    \"pod_type\": \"p1.x1\",\n",
    "                    \"metadata_config\": {\n",
    "                        \"indexed\": [\"file_type\", \"source\", \"chunk_index\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        while not pc.describe_index(config.index_name).status['ready']:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pc.Index(config.index_name)\n",
    "\n",
    "index = setup_pinecone_index()\n",
    "\n",
    "# === DOCUMENT READERS ===\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"Detect file encoding\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "def read_pdf(path):\n",
    "    \"\"\"Enhanced PDF reader with fallback methods\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try pdfplumber first (better for tables)\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to PyPDF2\n",
    "            with open(path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    return normalize_text(text) if text else None\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Enhanced text normalization\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace while preserving structure\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    \n",
    "    # Remove null bytes and other problematic characters\n",
    "    s = s.replace('\\x00', '').replace('\\xa0', ' ')\n",
    "    \n",
    "    # Normalize unicode\n",
    "    s = s.encode('utf-8', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Clean up lines\n",
    "    lines = []\n",
    "    for line in s.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and not line.isspace():\n",
    "            lines.append(line)\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def read_txt(path):\n",
    "    \"\"\"Enhanced text reader with encoding detection\"\"\"\n",
    "    try:\n",
    "        encoding = detect_encoding(path)\n",
    "        with open(path, \"r\", encoding=encoding, errors=\"ignore\") as f:\n",
    "            return normalize_text(f.read())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_json(path):\n",
    "    \"\"\"Enhanced JSON reader with nested structure handling\"\"\"\n",
    "    try:\n",
    "        encoding = detect_encoding(path)\n",
    "        with open(path, \"r\", encoding=encoding, errors=\"ignore\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Enhanced extraction with path preservation\n",
    "        extracted = extract_json_with_context(data)\n",
    "        return normalize_text(\"\\n\".join(extracted))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_json_with_context(obj, path=\"\"):\n",
    "    \"\"\"Extract text from JSON while preserving hierarchical context\"\"\"\n",
    "    texts = []\n",
    "    keys = {\"text\", \"content\", \"context\", \"body\", \"chunk\", \"page_content\", \n",
    "            \"data\", \"message\", \"description\", \"summary\"}\n",
    "    \n",
    "    def walk(o, current_path):\n",
    "        if isinstance(o, dict):\n",
    "            for k, v in o.items():\n",
    "                new_path = f\"{current_path}.{k}\" if current_path else k\n",
    "                if k.lower() in keys and isinstance(v, str) and v.strip():\n",
    "                    texts.append(f\"[{new_path}]: {v}\")\n",
    "                else:\n",
    "                    walk(v, new_path)\n",
    "        elif isinstance(o, list):\n",
    "            for i, item in enumerate(o):\n",
    "                walk(item, f\"{current_path}[{i}]\")\n",
    "        elif isinstance(o, str) and o.strip():\n",
    "            texts.append(o)\n",
    "    \n",
    "    walk(obj, path)\n",
    "    return texts\n",
    "\n",
    "def read_tsv(path):\n",
    "    \"\"\"Enhanced TSV reader with column context preservation\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "        \n",
    "        # Create semantic documents from rows\n",
    "        texts = []\n",
    "        for _, row in df.iterrows():\n",
    "            row_text = \" | \".join([f\"{col}: {val}\" for col, val in row.items() \n",
    "                                  if pd.notna(val) and str(val).strip()])\n",
    "            if row_text:\n",
    "                texts.append(row_text)\n",
    "        \n",
    "        return normalize_text(\"\\n\".join(texts))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# === ADVANCED TEXT PROCESSING ===\n",
    "class EnhancedTextProcessor:\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        self.bm25_index = None\n",
    "        self.corpus_for_bm25 = []\n",
    "        \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract named entities with confidence filtering\"\"\"\n",
    "        doc = nlp(text[:1000000])  # Limit for performance\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"EVENT\", \"LAW\"]:\n",
    "                entities.append(ent.text)\n",
    "        return list(set(entities))\n",
    "    \n",
    "    def extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Multi-method keyword extraction\"\"\"\n",
    "        keywords = set()\n",
    "        \n",
    "        # YAKE keywords\n",
    "        yake_kws = [kw for kw, _ in kw_extractor.extract_keywords(text)]\n",
    "        keywords.update(yake_kws[:5])\n",
    "        \n",
    "        # KeyBERT keywords\n",
    "        try:\n",
    "            keybert_kws = keybert_model.extract_keywords(\n",
    "                text, \n",
    "                keyphrase_ngram_range=(1, 3), \n",
    "                stop_words='english',\n",
    "                top_n=5\n",
    "            )\n",
    "            keywords.update([kw for kw, _ in keybert_kws])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return list(keywords)[:10]\n",
    "    \n",
    "    def generate_summary(self, text: str) -> str:\n",
    "        \"\"\"Generate concise summary using GPT-4\"\"\"\n",
    "        if len(text) < 100:\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # Truncate to avoid token limits\n",
    "            truncated = text[:2000]\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Create a 2-sentence summary focusing on key information.\"},\n",
    "                    {\"role\": \"user\", \"content\": truncated}\n",
    "                ],\n",
    "                max_tokens=100,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except:\n",
    "            # Fallback to extractive summary\n",
    "            sentences = text.split('.')[:2]\n",
    "            return '. '.join(sentences).strip()\n",
    "    \n",
    "    def create_sparse_vector(self, text: str) -> Dict[int, float]:\n",
    "        \"\"\"Create sparse vector for hybrid search\"\"\"\n",
    "        try:\n",
    "            # Use TF-IDF for sparse representation\n",
    "            tfidf_vector = self.tfidf_vectorizer.transform([text])\n",
    "            \n",
    "            # Convert to Pinecone sparse vector format\n",
    "            indices = tfidf_vector.nonzero()[1]\n",
    "            values = tfidf_vector.data\n",
    "            \n",
    "            sparse_dict = {int(idx): float(val) for idx, val in zip(indices, values)}\n",
    "            return sparse_dict\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "processor = EnhancedTextProcessor()\n",
    "\n",
    "# === INTELLIGENT CHUNKING ===\n",
    "class SemanticChunker:\n",
    "    def __init__(self):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            length_function=self.token_length,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def token_length(self, text: str) -> int:\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    def chunk_with_context(self, text: str, metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Create chunks with enhanced metadata\"\"\"\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        \n",
    "        enhanced_chunks = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            # Add context from surrounding chunks\n",
    "            context_before = chunks[idx-1][-100:] if idx > 0 else \"\"\n",
    "            context_after = chunks[idx+1][:100] if idx < len(chunks)-1 else \"\"\n",
    "            \n",
    "            enhanced_chunks.append({\n",
    "                \"text\": chunk,\n",
    "                \"context\": f\"{context_before} [CHUNK] {context_after}\",\n",
    "                \"chunk_index\": idx,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                **metadata\n",
    "            })\n",
    "        \n",
    "        return enhanced_chunks\n",
    "\n",
    "chunker = SemanticChunker()\n",
    "\n",
    "# === EMBEDDING GENERATION WITH CACHING ===\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self):\n",
    "        self.cache_file = os.path.join(config.cache_dir, \"embeddings_cache.pkl\")\n",
    "        self.cache = self.load_cache()\n",
    "    \n",
    "    def load_cache(self) -> Dict:\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                pass\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self):\n",
    "        with open(self.cache_file, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding with caching\"\"\"\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        \n",
    "        if text_hash in self.cache:\n",
    "            return self.cache[text_hash]\n",
    "        \n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=config.embedding_model,\n",
    "                input=text,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            self.cache[text_hash] = embedding\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Batch embedding generation with retry logic\"\"\"\n",
    "        embeddings = []\n",
    "        uncached_texts = []\n",
    "        uncached_indices = []\n",
    "        \n",
    "        # Check cache first\n",
    "        for i, text in enumerate(texts):\n",
    "            text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "            if text_hash in self.cache:\n",
    "                embeddings.append(self.cache[text_hash])\n",
    "            else:\n",
    "                embeddings.append(None)\n",
    "                uncached_texts.append(text)\n",
    "                uncached_indices.append(i)\n",
    "        \n",
    "        # Generate embeddings for uncached texts\n",
    "        if uncached_texts:\n",
    "            for i in range(0, len(uncached_texts), 20):  # Process in smaller batches\n",
    "                batch = uncached_texts[i:i+20]\n",
    "                retries = 3\n",
    "                \n",
    "                while retries > 0:\n",
    "                    try:\n",
    "                        response = client.embeddings.create(\n",
    "                            model=config.embedding_model,\n",
    "                            input=batch,\n",
    "                            encoding_format=\"float\"\n",
    "                        )\n",
    "                        \n",
    "                        for j, embedding_data in enumerate(response.data):\n",
    "                            idx = uncached_indices[i+j]\n",
    "                            embeddings[idx] = embedding_data.embedding\n",
    "                            # Cache the embedding\n",
    "                            text_hash = hashlib.md5(texts[idx].encode()).hexdigest()\n",
    "                            self.cache[text_hash] = embedding_data.embedding\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Batch embedding error: {e}, retrying...\")\n",
    "                        retries -= 1\n",
    "                        time.sleep(2)\n",
    "        \n",
    "        self.save_cache()\n",
    "        return embeddings\n",
    "\n",
    "embedder = EmbeddingGenerator()\n",
    "\n",
    "# === QUERY EXPANSION AND PROCESSING ===\n",
    "class QueryProcessor:\n",
    "    def expand_query_with_synonyms(self, query: str) -> str:\n",
    "        \"\"\"Expand query with WordNet synonyms\"\"\"\n",
    "        tokens = nltk.word_tokenize(query.lower())\n",
    "        expanded_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            expanded_tokens.append(token)\n",
    "            synsets = wordnet.synsets(token)\n",
    "            if synsets:\n",
    "                # Add first synonym from each synset\n",
    "                for synset in synsets[:2]:\n",
    "                    for lemma in synset.lemmas()[:2]:\n",
    "                        synonym = lemma.name().replace('_', ' ')\n",
    "                        if synonym != token:\n",
    "                            expanded_tokens.append(synonym)\n",
    "        \n",
    "        return ' '.join(expanded_tokens)\n",
    "    \n",
    "    def reformulate_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate multiple query perspectives using LLM\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Generate 3 alternative phrasings of the query. Return only the alternatives, one per line.\"},\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ],\n",
    "                max_tokens=150,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            alternatives = response.choices[0].message.content.strip().split('\\n')\n",
    "            return [query] + [alt.strip() for alt in alternatives if alt.strip()][:2]\n",
    "        except:\n",
    "            return [query]\n",
    "    \n",
    "    def extract_query_intent(self, query: str) -> Dict:\n",
    "        \"\"\"Extract intent and key components from query\"\"\"\n",
    "        doc = nlp(query)\n",
    "        \n",
    "        return {\n",
    "            \"entities\": [ent.text for ent in doc.ents],\n",
    "            \"keywords\": processor.extract_keywords(query),\n",
    "            \"expanded\": self.expand_query_with_synonyms(query),\n",
    "            \"alternatives\": self.reformulate_query(query)\n",
    "        }\n",
    "\n",
    "query_processor = QueryProcessor()\n",
    "\n",
    "# === HYBRID SEARCH WITH RERANKING ===\n",
    "class HybridSearchEngine:\n",
    "    def __init__(self):\n",
    "        self.processor = processor\n",
    "        self.embedder = embedder\n",
    "        self.query_processor = query_processor\n",
    "        self.cross_encoder = cross_encoder\n",
    "    \n",
    "    def dense_search(self, query: str, top_k: int = 50) -> List[Dict]:\n",
    "        \"\"\"Semantic similarity search\"\"\"\n",
    "        query_embedding = self.embedder.get_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return []\n",
    "        \n",
    "        results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"id\": match[\"id\"],\n",
    "                \"score\": match[\"score\"],\n",
    "                \"text\": match[\"metadata\"].get(\"text\", \"\"),\n",
    "                \"metadata\": match[\"metadata\"]\n",
    "            }\n",
    "            for match in results.get(\"matches\", [])\n",
    "        ]\n",
    "    \n",
    "    def keyword_search(self, query: str, documents: List[Dict], top_k: int = 30) -> List[Dict]:\n",
    "        \"\"\"BM25 keyword search on retrieved documents\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Create BM25 index from documents\n",
    "        corpus = [doc[\"text\"] for doc in documents]\n",
    "        tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        # Search with expanded query\n",
    "        expanded_query = self.query_processor.expand_query_with_synonyms(query)\n",
    "        query_tokens = expanded_query.lower().split()\n",
    "        \n",
    "        scores = bm25.get_scores(query_tokens)\n",
    "        \n",
    "        # Combine with original scores\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc[\"bm25_score\"] = scores[i]\n",
    "        \n",
    "        return sorted(documents, key=lambda x: x.get(\"bm25_score\", 0), reverse=True)[:top_k]\n",
    "    \n",
    "    def rerank_with_cross_encoder(self, query: str, documents: List[Dict], top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank using cross-encoder for better relevance\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Prepare pairs for cross-encoder\n",
    "        pairs = [[query, doc[\"text\"]] for doc in documents]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Add scores to documents\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc[\"rerank_score\"] = float(scores[i])\n",
    "        \n",
    "        # Sort by rerank score\n",
    "        reranked = sorted(documents, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "        \n",
    "        return reranked[:top_k]\n",
    "    \n",
    "    def reciprocal_rank_fusion(self, result_lists: List[List[Dict]], k: int = 60) -> List[Dict]:\n",
    "        \"\"\"Fuse multiple result lists using RRF\"\"\"\n",
    "        fused_scores = defaultdict(float)\n",
    "        all_docs = {}\n",
    "        \n",
    "        for results in result_lists:\n",
    "            for rank, doc in enumerate(results):\n",
    "                doc_id = doc[\"id\"]\n",
    "                all_docs[doc_id] = doc\n",
    "                fused_scores[doc_id] += 1.0 / (k + rank + 1)\n",
    "        \n",
    "        # Sort by fused score\n",
    "        sorted_ids = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)\n",
    "        \n",
    "        return [all_docs[doc_id] for doc_id in sorted_ids]\n",
    "    \n",
    "    def search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Main search pipeline with all optimizations\"\"\"\n",
    "        print(f\"Processing query: {query}\")\n",
    "        \n",
    "        # 1. Query understanding\n",
    "        query_intent = self.query_processor.extract_query_intent(query)\n",
    "        \n",
    "        # 2. Multi-query search\n",
    "        all_results = []\n",
    "        \n",
    "        # Search with original and alternative queries\n",
    "        for q in query_intent[\"alternatives\"][:2]:\n",
    "            results = self.dense_search(q, config.initial_retrieval_k)\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # 3. Fusion\n",
    "        fused_results = self.reciprocal_rank_fusion(all_results)[:config.rerank_k]\n",
    "        \n",
    "        # 4. Keyword reranking\n",
    "        keyword_reranked = self.keyword_search(query, fused_results, config.rerank_k)\n",
    "        \n",
    "        # 5. Cross-encoder reranking\n",
    "        final_results = self.rerank_with_cross_encoder(query, keyword_reranked, config.final_k)\n",
    "        \n",
    "        # 6. Add diversity using MMR\n",
    "        diverse_results = self.mmr_selection(query, final_results, lambda_param=0.7)\n",
    "        \n",
    "        return diverse_results\n",
    "    \n",
    "    def mmr_selection(self, query: str, documents: List[Dict], lambda_param: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"Maximal Marginal Relevance for diversity\"\"\"\n",
    "        if len(documents) <= 1:\n",
    "            return documents\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = np.array(self.embedder.get_embedding(query))\n",
    "        \n",
    "        # Get document embeddings\n",
    "        doc_embeddings = []\n",
    "        for doc in documents:\n",
    "            emb = self.embedder.get_embedding(doc[\"text\"])\n",
    "            if emb:\n",
    "                doc_embeddings.append(np.array(emb))\n",
    "            else:\n",
    "                doc_embeddings.append(np.zeros_like(query_embedding))\n",
    "        \n",
    "        doc_embeddings = np.array(doc_embeddings)\n",
    "        \n",
    "        # Calculate similarity to query\n",
    "        query_similarities = np.dot(doc_embeddings, query_embedding)\n",
    "        \n",
    "        selected = []\n",
    "        selected_indices = []\n",
    "        \n",
    "        # Select first document (highest relevance)\n",
    "        first_idx = np.argmax(query_similarities)\n",
    "        selected.append(documents[first_idx])\n",
    "        selected_indices.append(first_idx)\n",
    "        \n",
    "        # Select remaining documents\n",
    "        while len(selected) < min(len(documents), config.final_k):\n",
    "            remaining_indices = [i for i in range(len(documents)) if i not in selected_indices]\n",
    "            \n",
    "            if not remaining_indices:\n",
    "                break\n",
    "            \n",
    "            # Calculate MMR scores\n",
    "            mmr_scores = []\n",
    "            for idx in remaining_indices:\n",
    "                # Relevance to query\n",
    "                relevance = query_similarities[idx]\n",
    "                \n",
    "                # Max similarity to selected documents\n",
    "                max_sim = 0\n",
    "                for sel_idx in selected_indices:\n",
    "                    sim = np.dot(doc_embeddings[idx], doc_embeddings[sel_idx])\n",
    "                    max_sim = max(max_sim, sim)\n",
    "                \n",
    "                # MMR score\n",
    "                mmr = lambda_param * relevance - (1 - lambda_param) * max_sim\n",
    "                mmr_scores.append((idx, mmr))\n",
    "            \n",
    "            # Select document with highest MMR\n",
    "            best_idx = max(mmr_scores, key=lambda x: x[1])[0]\n",
    "            selected.append(documents[best_idx])\n",
    "            selected_indices.append(best_idx)\n",
    "        \n",
    "        return selected\n",
    "\n",
    "search_engine = HybridSearchEngine()\n",
    "\n",
    "# === MAIN PROCESSING PIPELINE ===\n",
    "def process_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Process a single file and return chunks\"\"\"\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # Read file based on extension\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \".json\":\n",
    "        text = read_json(file_path)\n",
    "    elif ext in [\".tsv\", \".csv\"]:\n",
    "        text = read_tsv(file_path)\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Create base metadata\n",
    "    base_metadata = {\n",
    "        \"source\": os.path.relpath(file_path, config.data_folder),\n",
    "        \"file_type\": ext[1:],\n",
    "        \"file_size\": os.path.getsize(file_path),\n",
    "        \"processing_timestamp\": time.time()\n",
    "    }\n",
    "    \n",
    "    # Generate document-level summary\n",
    "    doc_summary = processor.generate_summary(text[:3000])\n",
    "    base_metadata[\"document_summary\"] = doc_summary\n",
    "    \n",
    "    # Extract document-level entities\n",
    "    doc_entities = processor.extract_entities(text[:5000])\n",
    "    base_metadata[\"document_entities\"] = doc_entities[:20]\n",
    "    \n",
    "    # Create chunks with context\n",
    "    chunks = chunker.chunk_with_context(text, base_metadata)\n",
    "    \n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk_data in chunks:\n",
    "        chunk_text = chunk_data[\"text\"]\n",
    "        \n",
    "        # Skip if too short\n",
    "        if len(chunk_text) < 50:\n",
    "            continue\n",
    "        \n",
    "        # Generate chunk ID\n",
    "        chunk_hash = hashlib.sha256(chunk_text.encode()).hexdigest()[:16]\n",
    "        chunk_id = f\"{os.path.basename(file_path)}-{chunk_hash}\"\n",
    "        \n",
    "        # Extract chunk-level features\n",
    "        chunk_entities = processor.extract_entities(chunk_text)\n",
    "        chunk_keywords = processor.extract_keywords(chunk_text)\n",
    "        chunk_summary = processor.generate_summary(chunk_text)\n",
    "        \n",
    "        # Build chunk metadata\n",
    "        metadata = {\n",
    "            **chunk_data,\n",
    "            \"chunk_entities\": chunk_entities[:10],\n",
    "            \"chunk_keywords\": chunk_keywords[:10],\n",
    "            \"chunk_summary\": chunk_summary,\n",
    "            \"token_count\": chunker.token_length(chunk_text)\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "def upload_to_pinecone(chunks: List[Dict]):\n",
    "    \"\"\"Upload chunks to Pinecone with optimizations\"\"\"\n",
    "    if not chunks:\n",
    "        return 0\n",
    "    \n",
    "    # Generate embeddings in batch\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embeddings = embedder.get_embeddings_batch(texts)\n",
    "    \n",
    "    # Filter out failed embeddings\n",
    "    valid_chunks = []\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        if embedding:\n",
    "            valid_chunks.append({\n",
    "                \"id\": chunk[\"id\"],\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": {\n",
    "                    **chunk[\"metadata\"],\n",
    "                    \"text\": chunk[\"text\"]  # Store text in metadata for retrieval\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    if not valid_chunks:\n",
    "        return 0\n",
    "    \n",
    "    # Upload in batches\n",
    "    uploaded = 0\n",
    "    for i in range(0, len(valid_chunks), config.batch_size):\n",
    "        batch = valid_chunks[i:i+config.batch_size]\n",
    "        try:\n",
    "            response = index.upsert(vectors=batch)\n",
    "            uploaded += len(batch)\n",
    "            print(f\"  Uploaded {len(batch)} vectors (Total: {uploaded}/{len(valid_chunks)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Upload error: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return uploaded\n",
    "\n",
    "def process_directory_parallel(directory: str):\n",
    "    \"\"\"Process all files in directory using parallel processing\"\"\"\n",
    "    # Collect all files\n",
    "    files_to_process = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.txt', '.json', '.tsv', '.csv', '.pdf')):\n",
    "                files_to_process.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(files_to_process)} files to process\")\n",
    "    \n",
    "    # Process files in parallel\n",
    "    all_chunks = []\n",
    "    with ThreadPoolExecutor(max_workers=config.max_workers) as executor:\n",
    "        future_to_file = {executor.submit(process_file, f): f for f in files_to_process}\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                chunks = future.result()\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"  ✓ Processed {file_path}: {len(chunks)} chunks\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Upload all chunks\n",
    "    print(f\"\\nUploading {len(all_chunks)} total chunks to Pinecone...\")\n",
    "    total_uploaded = upload_to_pinecone(all_chunks)\n",
    "    \n",
    "    # Save TF-IDF model for sparse search\n",
    "    if all_chunks:\n",
    "        corpus = [c[\"text\"] for c in all_chunks]\n",
    "        processor.tfidf_vectorizer.fit(corpus)\n",
    "        \n",
    "        # Save the fitted vectorizer\n",
    "        import joblib\n",
    "        joblib.dump(processor.tfidf_vectorizer, os.path.join(config.cache_dir, 'tfidf_model.pkl'))\n",
    "        print(f\"  ✓ Saved TF-IDF model for sparse search\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing Complete!\")\n",
    "    print(f\"  - Files processed: {len(files_to_process)}\")\n",
    "    print(f\"  - Chunks created: {len(all_chunks)}\")\n",
    "    print(f\"  - Vectors uploaded: {total_uploaded}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return total_uploaded\n",
    "\n",
    "# === QUERY INTERFACE ===\n",
    "class RAGQueryEngine:\n",
    "    \"\"\"High-level interface for querying the RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.search_engine = search_engine\n",
    "        self.context_window = 8000  # tokens for context\n",
    "        \n",
    "    def query(self, question: str, return_sources: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute a RAG query with all optimizations\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            return_sources: Whether to return source documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, sources, and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Retrieve relevant documents\n",
    "        print(f\"\\nSearching for: {question}\")\n",
    "        results = self.search_engine.search(question)\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find relevant information to answer your question.\",\n",
    "                \"sources\": [],\n",
    "                \"search_time\": time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # 2. Build context from retrieved documents\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            chunk_text = result[\"text\"]\n",
    "            chunk_tokens = chunker.token_length(chunk_text)\n",
    "            \n",
    "            # Check if we have room for this chunk\n",
    "            if total_tokens + chunk_tokens > self.context_window:\n",
    "                break\n",
    "            \n",
    "            context_parts.append(f\"[Document {i+1}]\\n{chunk_text}\")\n",
    "            total_tokens += chunk_tokens\n",
    "            \n",
    "            # Collect source information\n",
    "            if return_sources:\n",
    "                sources.append({\n",
    "                    \"source\": result[\"metadata\"].get(\"source\", \"Unknown\"),\n",
    "                    \"chunk_index\": result[\"metadata\"].get(\"chunk_index\", 0),\n",
    "                    \"relevance_score\": result.get(\"rerank_score\", result.get(\"score\", 0)),\n",
    "                    \"summary\": result[\"metadata\"].get(\"chunk_summary\", \"\"),\n",
    "                    \"entities\": result[\"metadata\"].get(\"chunk_entities\", [])\n",
    "                })\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # 3. Generate answer using GPT-4\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
    "                        Follow these guidelines:\n",
    "                        1. Answer based ONLY on the information in the context\n",
    "                        2. If the context doesn't contain enough information, say so\n",
    "                        3. Be concise but comprehensive\n",
    "                        4. Cite document numbers when referencing specific information\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            answer = f\"Error generating answer: {str(e)}\"\n",
    "        \n",
    "        # 4. Prepare response\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "            \"search_time\": search_time,\n",
    "            \"documents_used\": len(context_parts),\n",
    "            \"total_retrieved\": len(results),\n",
    "            \"context_tokens\": total_tokens\n",
    "        }\n",
    "    \n",
    "    def query_with_feedback(self, question: str, feedback_callback=None) -> Dict[str, Any]:\n",
    "        \"\"\"Query with option for relevance feedback\"\"\"\n",
    "        result = self.query(question)\n",
    "        \n",
    "        if feedback_callback:\n",
    "            # Collect feedback on the answer\n",
    "            feedback = feedback_callback(result)\n",
    "            \n",
    "            # Store feedback for future improvements\n",
    "            self.store_feedback(question, result, feedback)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def store_feedback(self, question: str, result: Dict, feedback: Dict):\n",
    "        \"\"\"Store feedback for continuous improvement\"\"\"\n",
    "        feedback_data = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"relevance_score\": feedback.get(\"relevance\", 0),\n",
    "            \"helpful\": feedback.get(\"helpful\", False),\n",
    "            \"sources_accurate\": feedback.get(\"sources_accurate\", False)\n",
    "        }\n",
    "        \n",
    "        # Save to feedback file\n",
    "        feedback_file = os.path.join(config.cache_dir, \"feedback.jsonl\")\n",
    "        with open(feedback_file, \"a\") as f:\n",
    "            f.write(json.dumps(feedback_data) + \"\\n\")\n",
    "\n",
    "# === EVALUATION METRICS ===\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.query_engine = RAGQueryEngine()\n",
    "    \n",
    "    def evaluate_retrieval_quality(self, test_queries: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate retrieval quality with test queries\n",
    "        \n",
    "        Args:\n",
    "            test_queries: List of dicts with 'question' and 'expected_sources'\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            \"precision_at_k\": [],\n",
    "            \"recall_at_k\": [],\n",
    "            \"mrr\": [],  # Mean Reciprocal Rank\n",
    "            \"avg_response_time\": []\n",
    "        }\n",
    "        \n",
    "        for test_case in test_queries:\n",
    "            question = test_case[\"question\"]\n",
    "            expected = set(test_case.get(\"expected_sources\", []))\n",
    "            \n",
    "            # Get results\n",
    "            result = self.query_engine.query(question)\n",
    "            retrieved = set([s[\"source\"] for s in result[\"sources\"]])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if expected:\n",
    "                precision = len(expected & retrieved) / len(retrieved) if retrieved else 0\n",
    "                recall = len(expected & retrieved) / len(expected) if expected else 0\n",
    "                \n",
    "                metrics[\"precision_at_k\"].append(precision)\n",
    "                metrics[\"recall_at_k\"].append(recall)\n",
    "            \n",
    "            metrics[\"avg_response_time\"].append(result[\"search_time\"])\n",
    "        \n",
    "        # Calculate averages\n",
    "        return {\n",
    "            \"avg_precision\": np.mean(metrics[\"precision_at_k\"]) if metrics[\"precision_at_k\"] else 0,\n",
    "            \"avg_recall\": np.mean(metrics[\"recall_at_k\"]) if metrics[\"recall_at_k\"] else 0,\n",
    "            \"avg_response_time\": np.mean(metrics[\"avg_response_time\"]),\n",
    "            \"total_queries\": len(test_queries)\n",
    "        }\n",
    "    \n",
    "    def evaluate_answer_quality(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"Evaluate answer quality using LLM-as-judge\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for test in test_cases:\n",
    "            question = test[\"question\"]\n",
    "            expected_answer = test.get(\"expected_answer\", \"\")\n",
    "            \n",
    "            # Get RAG answer\n",
    "            result = self.query_engine.query(question)\n",
    "            generated_answer = result[\"answer\"]\n",
    "            \n",
    "            # Use GPT-4 to evaluate\n",
    "            try:\n",
    "                eval_response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"Evaluate the quality of the generated answer compared to the expected answer.\n",
    "                            Score from 1-5:\n",
    "                            1 = Completely wrong or irrelevant\n",
    "                            2 = Partially correct but missing key information\n",
    "                            3 = Mostly correct with minor issues\n",
    "                            4 = Correct and comprehensive\n",
    "                            5 = Perfect answer with all relevant details\n",
    "                            \n",
    "                            Return only the numeric score.\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"\"\"Question: {question}\n",
    "                            \n",
    "                            Expected Answer: {expected_answer}\n",
    "                            \n",
    "                            Generated Answer: {generated_answer}\n",
    "                            \n",
    "                            Score:\"\"\"\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=10\n",
    "                )\n",
    "                \n",
    "                score = int(eval_response.choices[0].message.content.strip())\n",
    "                scores.append(score)\n",
    "                \n",
    "            except:\n",
    "                scores.append(0)\n",
    "        \n",
    "        return {\n",
    "            \"avg_quality_score\": np.mean(scores) if scores else 0,\n",
    "            \"score_distribution\": dict(zip(*np.unique(scores, return_counts=True))) if scores else {},\n",
    "            \"total_evaluated\": len(scores)\n",
    "        }\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Enhanced RAG Pipeline\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"index\", \"query\", \"evaluate\"], \n",
    "                       default=\"index\", help=\"Operation mode\")\n",
    "    parser.add_argument(\"--question\", type=str, help=\"Question for query mode\")\n",
    "    parser.add_argument(\"--eval-file\", type=str, help=\"JSON file with test cases for evaluation\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.mode == \"index\":\n",
    "        # Process and index documents\n",
    "        print(\"Starting document processing and indexing...\")\n",
    "        print(f\"Data directory: {config.data_folder}\")\n",
    "        print(f\"Using embedding model: {config.embedding_model}\")\n",
    "        print(f\"Batch size: {config.batch_size}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_uploaded = process_directory_parallel(config.data_folder)\n",
    "        \n",
    "        print(f\"\\nTotal processing time: {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Embeddings cache saved to: {config.cache_dir}\")\n",
    "        \n",
    "    elif args.mode == \"query\":\n",
    "        # Query mode\n",
    "        if not args.question:\n",
    "            # Interactive mode\n",
    "            print(\"RAG Query Interface (type 'exit' to quit)\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            query_engine = RAGQueryEngine()\n",
    "            \n",
    "            while True:\n",
    "                question = input(\"\\nEnter your question: \").strip()\n",
    "                \n",
    "                if question.lower() == 'exit':\n",
    "                    break\n",
    "                \n",
    "                if not question:\n",
    "                    continue\n",
    "                \n",
    "                result = query_engine.query(question)\n",
    "                \n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(\"ANSWER:\")\n",
    "                print(result[\"answer\"])\n",
    "                \n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Sources used: {result['documents_used']}\")\n",
    "                print(f\"Search time: {result['search_time']:.2f}s\")\n",
    "                \n",
    "                if result[\"sources\"]:\n",
    "                    print(\"\\nTop sources:\")\n",
    "                    for i, source in enumerate(result[\"sources\"][:3]):\n",
    "                        print(f\"  {i+1}. {source['source']} (relevance: {source['relevance_score']:.3f})\")\n",
    "                        if source['entities']:\n",
    "                            print(f\"     Entities: {', '.join(source['entities'][:5])}\")\n",
    "        else:\n",
    "            # Single query\n",
    "            query_engine = RAGQueryEngine()\n",
    "            result = query_engine.query(args.question)\n",
    "            \n",
    "            print(f\"\\nQuestion: {args.question}\")\n",
    "            print(f\"\\nAnswer: {result['answer']}\")\n",
    "            print(f\"\\nSearch completed in {result['search_time']:.2f} seconds\")\n",
    "            print(f\"Documents used: {result['documents_used']}/{result['total_retrieved']}\")\n",
    "    \n",
    "    elif args.mode == \"evaluate\":\n",
    "        # Evaluation mode\n",
    "        if not args.eval_file:\n",
    "            print(\"Please provide an evaluation file with --eval-file\")\n",
    "        else:\n",
    "            print(f\"Running evaluation from {args.eval_file}\")\n",
    "            \n",
    "            with open(args.eval_file, 'r') as f:\n",
    "                test_cases = json.load(f)\n",
    "            \n",
    "            evaluator = RAGEvaluator()\n",
    "            \n",
    "            # Evaluate retrieval\n",
    "            if \"retrieval_tests\" in test_cases:\n",
    "                print(\"\\nEvaluating retrieval quality...\")\n",
    "                retrieval_metrics = evaluator.evaluate_retrieval_quality(test_cases[\"retrieval_tests\"])\n",
    "                print(f\"  Average Precision: {retrieval_metrics['avg_precision']:.3f}\")\n",
    "                print(f\"  Average Recall: {retrieval_metrics['avg_recall']:.3f}\")\n",
    "                print(f\"  Average Response Time: {retrieval_metrics['avg_response_time']:.2f}s\")\n",
    "            \n",
    "            # Evaluate answer quality\n",
    "            if \"answer_tests\" in test_cases:\n",
    "                print(\"\\nEvaluating answer quality...\")\n",
    "                answer_metrics = evaluator.evaluate_answer_quality(test_cases[\"answer_tests\"])\n",
    "                print(f\"  Average Quality Score: {answer_metrics['avg_quality_score']:.2f}/5\")\n",
    "                print(f\"  Score Distribution: {answer_metrics['score_distribution']}\")\n",
    "    \n",
    "    print(\"\\n✨ Operation completed successfully!\")\n",
    "    \n",
    "    # Save embedding cache\n",
    "    embedder.save_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
