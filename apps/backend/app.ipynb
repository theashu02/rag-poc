{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e03236-6d23-41ad-8eb8-79849a77715f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# perplixity script with enhancements\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import spacy\n",
    "import yake\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\", \"YOUR_PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX\", \"YOUR_INDEX_NAME\")\n",
    "data_folder = os.getenv(\"DATA_DIR\", r\"YOUR_LOCAL_DATA_DIRECTORY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")  # 3072 dims for accuracy\n",
    "pinecone_environment = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east-1\")\n",
    "\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\", 100))\n",
    "start_batch = int(os.getenv(\"START_BATCH\", 0))\n",
    "\n",
    "# =========================\n",
    "# INIT CLIENTS\n",
    "# =========================\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # NER model\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "\n",
    "# Create Pinecone index if not exists\n",
    "index_list = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "if index_name not in index_list:\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        cloud=\"aws\",\n",
    "        region=pinecone_environment,\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print(f\"Using existing index: {index_name}\")\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    return \"\\n\".join(line.strip() for line in s.split(\"\\n\") if line.strip())\n",
    "\n",
    "def read_txt(file_path: str) -> str | None:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return normalize_text(f.read())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_json(file_path: str) -> str | None:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            data = json.load(f)\n",
    "        preferred_keys = {\"text\", \"content\", \"context\", \"body\", \"chunk\", \"page_content\", \"data\", \"message\"}\n",
    "        strings = []\n",
    "\n",
    "        def walk(o, parent_key=None):\n",
    "            if isinstance(o, dict):\n",
    "                for k, v in o.items():\n",
    "                    walk(v, k)\n",
    "            elif isinstance(o, list):\n",
    "                for item in o:\n",
    "                    walk(item, parent_key)\n",
    "            elif isinstance(o, str) and o.strip():\n",
    "                if parent_key and parent_key.lower() in preferred_keys:\n",
    "                    strings.append(o)\n",
    "                else:\n",
    "                    strings.append(o)\n",
    "\n",
    "        walk(data)\n",
    "        return normalize_text(\"\\n\".join(strings)) if strings else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_tsv(file_path: str) -> str | None:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", dtype=str, encoding=\"utf-8\", error_bad_lines=False)\n",
    "        text = \"\\n\".join(df.fillna(\"\").astype(str).agg(\" \".join, axis=1))\n",
    "        return normalize_text(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def iter_data_files(root_dir: str):\n",
    "    supported_ext = (\".txt\", \".json\", \".tsv\")\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for name in filenames:\n",
    "            if name.lower().endswith(supported_ext):\n",
    "                yield os.path.join(dirpath, name)\n",
    "\n",
    "def extract_entities(text: str):\n",
    "    doc = nlp(text)\n",
    "    return list({ent.text for ent in doc.ents})\n",
    "\n",
    "def extract_keywords(text: str):\n",
    "    return [kw for kw, _ in kw_extractor.extract_keywords(text)]\n",
    "\n",
    "def normalize_vector(vec):\n",
    "    vec = np.array(vec, dtype=np.float32)\n",
    "    return (vec / np.linalg.norm(vec)).tolist()\n",
    "\n",
    "def get_embeddings_batch(texts, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=embedding_model, input=texts)\n",
    "            return [d.embedding for d in resp.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding batch failed: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# CHUNKING\n",
    "# =========================\n",
    "splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=100, encoding_name=\"cl100k_base\")\n",
    "\n",
    "all_chunks = []\n",
    "seen_hashes = set()\n",
    "\n",
    "print(f\"Scanning dataset in {data_folder}...\")\n",
    "for file_path in tqdm(list(iter_data_files(data_folder))):\n",
    "    if file_path.lower().endswith(\".txt\"):\n",
    "        text = read_txt(file_path)\n",
    "    elif file_path.lower().endswith(\".json\"):\n",
    "        text = read_json(file_path)\n",
    "    elif file_path.lower().endswith(\".tsv\"):\n",
    "        text = read_tsv(file_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        h = hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "\n",
    "        metadata = {\n",
    "            \"source\": os.path.relpath(file_path, data_folder).replace(\"\\\\\", \"/\"),\n",
    "            \"chunk_index\": i,\n",
    "            \"file_type\": os.path.splitext(file_path)[1][1:],\n",
    "            \"entities\": extract_entities(chunk),\n",
    "            \"keywords\": extract_keywords(chunk),\n",
    "            \"chunk_length\": len(chunk)\n",
    "        }\n",
    "\n",
    "        all_chunks.append({\n",
    "            \"id\": f\"{os.path.basename(file_path)}-{h[:12]}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(all_chunks)} chunks.\")\n",
    "\n",
    "# =========================\n",
    "# EMBEDDING + UPLOAD\n",
    "# =========================\n",
    "total_batches = (len(all_chunks) + batch_size - 1) // batch_size\n",
    "uploaded_vectors = 0\n",
    "\n",
    "for idx in range(start_batch, total_batches):\n",
    "    batch = all_chunks[idx * batch_size : (idx + 1) * batch_size]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "\n",
    "    embeddings = get_embeddings_batch(texts)\n",
    "    if not embeddings:\n",
    "        print(\"Skipping batch due to embedding failure.\")\n",
    "        continue\n",
    "\n",
    "    vectors = []\n",
    "    for item, emb in zip(batch, embeddings):\n",
    "        vectors.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"values\": normalize_vector(emb),\n",
    "            \"metadata\": {**item[\"metadata\"], \"text\": item[\"text\"]}\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        index.upsert(vectors=vectors)\n",
    "        uploaded_vectors += len(vectors)\n",
    "        print(f\"Uploaded {len(vectors)} vectors (Total: {uploaded_vectors})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upsert batch {idx+1}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Total vectors uploaded: {uploaded_vectors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4adb6ee-7455-43b3-b03f-7df46ae03428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
